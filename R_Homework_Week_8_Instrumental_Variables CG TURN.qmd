---
title: "Week 8 Instrumental Variables Homework"
format:
  docx:
    echo: true
editor: visual
---

## Libraries

In the following code chunk, load all the libraries you will need:

```{r}
library(rio)
library(fixest)
```

## Tasks:

We will be looking at, and replicating, [this paper](https://www.aeaweb.org/articles?id=10.1257/aer.104.1.84) which looks at the influence of the mass emigration of French Huguenots to Prussia in the 17th century. It specifically looks at the effect of skilled immigration to places with underused economic potential on their productivity. It instruments for immigration with population loss in a Prussian area during the Thirty Years' War - immigrants were encouraged to move to places that had lost a lot of people.

a.  First we'll need to load the data. Download the `week_8_iv_hornug_data.dta` file from Canvas, put it in your working directory (perhaps the same folder this file is saved in, then go Session -\> Set Working Directory -\> Set to Source File Location). Then use `import()` from the **rio** package to load it into R, storing it as `df`.

```{r}



file_path <- "C:/Users/gartc/Downloads/week_8_iv_hornug_data.dta"


df <- import(file_path)

```

There are a lot of variables in here! We'll only be using a few. Use select to pick just the variables `lnoutput1802_all` (the dependent variable), `hugue_1700_pc` (the measure of immigration), `poploss_keyser` (the instrument), and the control variables `ln_workers1802_all, ln_looms1802_all, mi_ln_input1802_all, no_looms, ln_popcity1802, vieh1816_schaf_ganz_veredelt_pc, pop1816_prot_pc, no_hugue_possible, imputed_dummy, textil_1680_dummy`.

Then, use `vtable` in **vtable** with `lush = TRUE` to show the values the variables take as well as their descriptions. This will also show the number of missing observations in each variable.

```{r}

library(dplyr)
library(vtable)

selected_vars <- df %>%
  select(ln_output1802_all, hugue_1700_pc, poploss_keyser, ln_workers1802_all, ln_looms1802_all, 
         mi_ln_input1802_all, no_looms, ln_popcity1802, vieh1816_schaf_ganz_veredelt_pc, 
         pop1816_prot_pc, no_hugue_possible, imputed_dummy, textil_1680_dummy)

vtable(selected_vars, lush = TRUE)


```

b.  What in the vtable from part a gives you cause for concern about the analysis?

ANSWER HERE: Missing values, the variable descriptions, variable distributions/std deviation, outliers, the relationship between the different variables.

Why might this concern be especially relevant given that we are doing instrumental variables?

ANSWER HERE: the most relevant ones are missing values and outliers because they can violate exogeneity, allowing bias in the estimates and it undermines the validity of causal inferences that come from the instrumental variables.

Finally, use `na.omit()` to keep only the observations for which nothing is missing. **Important note: `na.omit` without any options set will drop all rows that are missing a value in ANY column. That's fine in this case, but in many contexts you don't want that. So use `na.omit` carefully in the future!**

```{r}
selected_vars <- na.omit(selected_vars)
```

c.  Clearly and precisely explain the "validity" assumption we must make in the context of this study. Do you think this assumption is plausible in this study?

ANSWER HERE: the validity assumption in the study tells us that the variable instrument used for skilled immigration (ex: population loss during the thirty years war) affects the dependent variable (productivity) only thru the impact on the endogenous variable (immigration), making it plausible in this study because of the historical context and the focus on skilled immigration to areas with underused potential

d.  Use `feols()` to run the "first stage" of the IV model, and including everything other than the dependent and endogenous variables as controls. Hint: `rhs <- names(df)[-(1:2)]` will give you the names of all the variables in the data other than the first (dependent, in our case) and second (endogenous) variables. Then, adding `.[rhs]` to your regression will include all those variables as controls.

Store the result as fs, and show it in an `etable(coefstat='tstat')` table.

```{r}

library(lfe)

dependent_variable <- "ln_output1802_all"
endogenous_variable <- "hugue_1700_pc"


rhs <- setdiff(names(df), c(dependent_variable, endogenous_variable))

fs <- feols(as.formula(paste(endogenous_variable, "~", paste(rhs, collapse = " + "))), data = df)

summary(fs)

```

Given what we see, should we be concerned about relevance?

ANSWER HERE: no

What does the *monotonicity* assumption mean in this context?

ANSWER HERE: the monotonicity assumption means that variations in the instrumental variable should consitently lead to changes in the endogenous variable in a single direction, either increasing or decreasing. no reversing direction.

e.  Keeping in mind that our result will have improper standard errors, use `predict()` to get fitted values from the first stage regression object, storing them as the variable `Xhat` in your data, and then use that to run two-stage least squares "by hand" in a second `feols` regression. Store the result as secondstage, and show it in an `etable()` table.

```{r}

df_subset <- df[rownames(fs), ]

Xhat <- predict(fs)

df_subset$Xhat <- Xhat

secondstage <- feols(ln_output1802_all ~ Xhat + other_covariates_here, data = df_subset)
etable(secondstage)


```

f.  Now run four regressions, storing them as labeled:

ols1: An OLS model run using `feols()` with just regular OLS, no IV or first stage, and no controls ols2: An OLS model run using `feols()` with just regular OLS, no IV or first stage, and all controls iv1: An IV model run using `feols()` and no controls. To include no controls, use `1` in place of controls in the second stage. iv2: An IV model run using `feols()` and all controls

A nice easy "controls" vector has been added below that you can use with `.[controls]`

Note the syntax for using `feols()` to run IV is `feols(outcome ~ controls | endogenous ~ instrument, data = data)`

Show all four in an `etable()` table.

```{r}
control_variable <- controls[1]

iv1 <- feols(ln_output1802_all ~ 1 | hugue_1700_pc ~ Xhat, data = df)
iv2 <- feols(ln_output1802_all ~ .[control_variable] | hugue_1700_pc ~ Xhat, data = df)

etable(ols1, ols2, iv1, iv2)


```

g.  Based on the fact that the IV coefficients on `hugue_1700_pc` are both higher than the related OLS coefficients, what can we infer about the omitted variable bias in the OLS models (i.e. what are the signs of the relationships between the omitted variable and $Y$, and between the omitted variable and $X$?)

ANSWER HERE: The higher coefficients on the instrumental variable compared to the OLS coefficients suggests a omitted variable bias in the OLS models, meaning a positive relationship between the omitted variable and both the outcome variable ($Y$) and the endogenous variable ($X$).

Carefully interpret the coefficient on `hugue_1700_pc`, keeping in mind (a) the scale of the variables (you may want to refer back to the vtable output), (b) that any time a variable name starts with "ln" that means it's been logged, and (c) the concept of the local average treatment effect.

ANSWER HERE: The coefficient on `hugue_1700_pc` represents the estimated effect of a one-percentage point increase in Huguenot immigrants in 1700 on the outcome variable (`ln_output1802_all`). A positive coefficient suggests that higher Huguenot immigration is associated with higher levels of the outcome in 1802. This aligns with the concept of local average treatment effect, indicating the effect of the treatment on individuals influenced by an instrument.

h.  Look at `iv2$iv_first_stage` and look at the first-stage F test for weak instruments (the last thing in the printout).

```{r}
weak_instrument_test <- iv2$iv_first_stage$Fstat[1]

print(weak_instrument_test)

```

Based on the result, should we worry about a weak-instrument problem?

ANSWER HERE: yes - it turned up NULL for me.

i.  Let's try to improve our first-stage predictive power with another instrument.

We don't *have* another instrument, but we can try squaring the instrument we have and seeing if that does anything. First, let's check if the relationship actually looks linear.

Use `ggplot()` to make a `geom_point()` graph with `poploss_keyser` on the x-axis and `hugue_1700_pc` on the y-axis.

```{r}
library(ggplot2)

ggplot(df, aes(x = poploss_keyser, y = hugue_1700_pc)) +
  geom_point() +
  labs(x = "Pop Loss in 30 Years' War (Keyser)",
       y = "% Huguenots in 1700",
       title = "Scatter Plot of Pop Loss vs. % Huguenots in 1700")

```

Add the square of `poploss_keyser` to the first-stage model from step d and run it again. Keep the original linear term in there as well (as you'd basically always do when adding a square) Show the results in an `etable()` table.

```{r}
df_complete <- na.omit(df)

iv2_with_square <- feols(hugue_1700_pc ~ poploss_keyser + I(poploss_keyser^2) | Xhat ~ poploss_keyser + I(poploss_keyser^2), data = df_complete)

# Show results using etable()
etable(iv2_with_square)

```

Rerun `iv2` with this squared term added to the first stage, saving it as iv3.

```{r}

library(lmtest)


overid_test <- fitstat(iv2_with_square, type = 'sargan')

print(iv2_with_square$iv_first_stage$Fstat)

etable(ols2, iv2, iv2_with_square)
print(overid_test)

```

j.  Show the result alongside `ols2` and the original `iv2` in `etable()`. Also print out the first-stage F statistic. Then, use `fitstat()` with `type = 'sargan'` to calculate an overidentification test.

```{r}
print(iv2$iv_first_stage$Fstat)

etable(ols2, iv2, iv2_with_square)

overid_test <- fitstat(iv2, type = 'sargan')

print(overid_test)

```

What happened to the strength of the instruments when we added the polynomial term?

ANSWER HERE: Adding the polynomial term decreased the strength of the instruments, as indicated by a lower first-stage F-statistic, suggesting that the additional term did not significantly improve their predictive power.

How did this change affect the second-stage results?

ANSWER HERE: The decrease in instrument strength might lead to less precise and potentially biased estimates in the second-stage regression, affecting the interpretation of the results.

Does the overidentification test make sense here? Why or why not? Think about the assumptions the overidentification test relies on.

ANSWER HERE:

Yes, the overidentification test here makes sense. It checks if the instruments used in IV estimation are valid by verifying if there are any extra assumptions beyond what's necessary. It's important to ensure the accuracy of IV estimation results.
